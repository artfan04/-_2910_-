"use strict";
var __addDisposableResource = (this && this.__addDisposableResource) || function (env, value, async) {
    if (value !== null && value !== void 0) {
        if (typeof value !== "object" && typeof value !== "function") throw new TypeError("Object expected.");
        var dispose, inner;
        if (async) {
            if (!Symbol.asyncDispose) throw new TypeError("Symbol.asyncDispose is not defined.");
            dispose = value[Symbol.asyncDispose];
        }
        if (dispose === void 0) {
            if (!Symbol.dispose) throw new TypeError("Symbol.dispose is not defined.");
            dispose = value[Symbol.dispose];
            if (async) inner = dispose;
        }
        if (typeof dispose !== "function") throw new TypeError("Object not disposable.");
        if (inner) dispose = function() { try { inner.call(this); } catch (e) { return Promise.reject(e); } };
        env.stack.push({ value: value, dispose: dispose, async: async });
    }
    else if (async) {
        env.stack.push({ async: true });
    }
    return value;
};
var __disposeResources = (this && this.__disposeResources) || (function (SuppressedError) {
    return function (env) {
        function fail(e) {
            env.error = env.hasError ? new SuppressedError(e, env.error, "An error was suppressed during disposal.") : e;
            env.hasError = true;
        }
        var r, s = 0;
        function next() {
            while (r = env.stack.pop()) {
                try {
                    if (!r.async && s === 1) return s = 0, env.stack.push(r), Promise.resolve().then(next);
                    if (r.dispose) {
                        var result = r.dispose.call(r.value);
                        if (r.async) return s |= 2, Promise.resolve(result).then(next, function(e) { fail(e); return next(); });
                    }
                    else s |= 1;
                }
                catch (e) {
                    fail(e);
                }
            }
            if (s === 1) return env.hasError ? Promise.reject(env.error) : Promise.resolve();
            if (env.hasError) throw env.error;
        }
        return next();
    };
})(typeof SuppressedError === "function" ? SuppressedError : function (error, suppressed, message) {
    var e = new Error(message);
    return e.name = "SuppressedError", e.error = error, e.suppressed = suppressed, e;
});
Object.defineProperty(exports, "__esModule", { value: true });
exports.getPartialAudioData = void 0;
const mediabunny_1 = require("mediabunny");
// Audio frames might have dependencies on previous and next frames so we need to decode a bit more and then discard it.
// The worst case seems to be FLAC files with a 65'535 sample window, which would be 1486.0ms at 44.1Khz.
// So let's set a threshold of 1.5 seconds.
const EXTRA_THRESHOLD_IN_SECONDS = 1.5;
const getPartialAudioData = async ({ fromSeconds, toSeconds, channelIndex, signal, src, isMatroska, }) => {
    const env_1 = { stack: [], error: void 0, hasError: false };
    try {
        if (signal.aborted) {
            throw new Error('Operation was aborted');
        }
        const audioSamples = [];
        // matroska must be decoded from the start due to limitation
        // https://www.remotion.dev/docs/media/support#matroska-limitation
        // Also request extra data beforehand to handle audio frame dependencies
        const actualFromSeconds = isMatroska
            ? 0
            : Math.max(0, fromSeconds - EXTRA_THRESHOLD_IN_SECONDS);
        const source = new mediabunny_1.UrlSource(src);
        const input = __addDisposableResource(env_1, new mediabunny_1.Input({
            formats: mediabunny_1.ALL_FORMATS,
            source,
        }), false);
        const track = await input.getPrimaryAudioTrack();
        if (!track) {
            throw new Error('No audio track found');
        }
        // mediabunny docs: constructing the sink is virtually free and does not perform any media data reads.
        const sink = new mediabunny_1.AudioBufferSink(track);
        const iterator = sink.buffers(actualFromSeconds, toSeconds);
        for await (const { buffer, timestamp, duration } of iterator) {
            if (signal.aborted) {
                break;
            }
            const channelData = buffer.getChannelData(channelIndex);
            const bufferStartSeconds = timestamp;
            const bufferEndSeconds = timestamp + duration;
            const overlapStartSecond = Math.max(bufferStartSeconds, fromSeconds);
            const overlapEndSecond = Math.min(bufferEndSeconds, toSeconds);
            if (overlapStartSecond >= overlapEndSecond) {
                continue;
            }
            const startSampleInBuffer = Math.floor((overlapStartSecond - bufferStartSeconds) * buffer.sampleRate);
            const endSampleInBuffer = Math.ceil((overlapEndSecond - bufferStartSeconds) * buffer.sampleRate);
            const trimmedData = channelData.slice(startSampleInBuffer, endSampleInBuffer);
            audioSamples.push(trimmedData);
        }
        await iterator.return();
        const totalSamples = audioSamples.reduce((sum, sample) => sum + sample.length, 0);
        const result = new Float32Array(totalSamples);
        let offset = 0;
        for (const audioSample of audioSamples) {
            result.set(audioSample, offset);
            offset += audioSample.length;
        }
        return result;
    }
    catch (e_1) {
        env_1.error = e_1;
        env_1.hasError = true;
    }
    finally {
        __disposeResources(env_1);
    }
};
exports.getPartialAudioData = getPartialAudioData;
